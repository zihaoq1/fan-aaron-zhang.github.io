<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Research Projects - Dr Fan (Aaron) Zhang</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/5.0.0/normalize.min.css">
    <link rel="stylesheet" href="./css/style.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
</head>

<body id="page-top">
    <div class="page-container">
        <div class="inner">
            <div class="container">
                <div class="row">
                    <div class="row vertical-align">
                        <div class="col-md-3">
                            <div class="title">
							<br>
							<br>
                                <p><a href="index.htm"><img src="Fan.jpg" width="100%" height="20%" class="center" /></a></p>
                            </div>
                        </div>
                        <div class="col-md-5">
                            <div class="title">
							<br>
							<br>
                                <h1>Dr Fan (Aaron) Zhang </h1>
								
								<b><font SIZE="4">Research Fellow</font></b><br>
								in Image and Video Communication<br>
								<br>
								1.23, 1 Cathedral Square<br>
                                University of Bristol<br>
                                Bristol BS1 5DD, United Kingdom<br>
                                fan.zhang@bristol.ac.uk<br>
								
								<ul class="list-inline list-social-icons mb-0">
								<br>
                                    <li class="list-inline-item">
                                        <a href="https://scholar.google.com/citations?hl=en&user=BBujJNcAAAAJ">
                                            <span class="fa-stack fa-lg">
                                                <i class="ai ai-google-scholar-square ai-2x"></i>
                                            </span>
                                        </a>
                                    </li>
<!--                                     <li class="list-inline-item">
                                        <a href="https://www.linkedin.com/in/fan-zhang-b32ba430/">
                                            <span class="fa-stack fa-lg">
												<i class="fa fa-square fa-stack-2x"></i>
                                                <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                                            </span>
                                        </a>
                                    </li> -->
                                    <li class="list-inline-item">
                                        <a href="CV_2021.pdf">
                                            <span class="fa-stack fa-lg">
                                                <i class="ai ai-cv-square ai-2x"></i>
                                            </span>
                                        </a>
                                    </li>
                                </ul>
                            </div>
                        </div>
                        <div class="col-md-4">
						<br>
						<br>
						<br>
						    <p><a href='https://www.bristol.ac.uk'><img src="uob-logo.svg" width="70%" height="20%" alt=""></a></p>
                            <p><a href='https://www.bristol.ac.uk/vision-institute'><img src="bvilogo.png" width="65%" height="20%" alt=""></a></p>										
                            <p><a href='https://vilab.blogs.bristol.ac.uk'><img src="VILogo.jpg" width="80%" height="20%" alt=""></a></p>


                        </div>
                    </div>
                </div>
            </div>
			<br>
            <hr />
            <div class="container-md">
                <h2 id="projects">Research Projects</h2>

			<br>
            <div class="researchlist">
			<font size=+1>
			<b><font SIZE="4">Deep Video Coding</font></b>
			</font>
			<br>
			<ul>
			<li><a href="CVE-GAN/">CVEGAN: A Perceptually-inspired GAN for Compressed Video Enhancement</a>
                    <p>CVEGAN is a new Generative Adversarial Network for Compressed Video quality Enhancement (CVEGAN). The generator benefits from the use of a novel Mul2Res block (with multiple levels of residual learning branches), an enhanced residual non-local block (ERNB) and an enhanced convolutional block attention module (ECBAM). The ERNB has also been employed in the discriminator to improve the representational capability. The training strategy has also been re-designed specifically for video compression applications, to employ a relativistic sphere GAN (ReSphereGAN) training methodology together with new perceptual loss functions. </p></li>
								<li><a href="DASA/">Learning-optimal Deep Visual Compression</a>
                    <p>Deep Learning systems offer state-of-the-art performance in image analysis, outperforming conventional methods. Such systems offer huge potential across military and commercial domains including: human/target detection and recognition and spatial localization/mapping. However, heavy computational requirements limit their exploitation in surveillance applications, particularly airborne, where low-power embedded processing and limited bandwidth are common constraints. This project is to explore deep learning performance whilst reducing processing and communication overheads, by developing learning-optimal compression schemes trained in conjunction with detection networks. </p></li>
			<li><a href="CNN-PP/">Video Compression with CNN-based Post Processing</a>
                    <p>This is a new CNN-based post-processing approach, which has been integrated with two state-of-the-art coding standards, VVC and AV1. It offers consistent coding gains on all tested sequences at various spatial resolutions, with average bit rate savings of 4.0% and 5.8% against original VVC and AV1 respectively (based on the assessment of PSNR). This network has also been trained with perceptually inspired loss functions, swhich have further improved reconstruction quality based on perceptual quality assessment (VMAF), with average coding gains of 13.9% over VVC and 10.5% against AV1.</p></li>
					                <li><a href="ViSTRA/">ViSTRA: Video Compression based on Resolution Adaptation</a>
                    <p>ViSTRA a new video compression framework which exploits adaptation of spatial/temporal resolution and effective bit depth, down-sampling these parameters at the encoder based on perceptual criteria, and up-sampling at the decoder using a deep convolution neural network.</p></li>
						<font size=+1>
			</ul>
			<b><font SIZE="4">Video Quality Assessment</font></b>
			</font>
			<br>
			<ul>
			<li>    <a href="BVI-SR/">BVI-SR: A Study of Subjective Video Quality at Various Spatial Resolutions</a>
                    <p>BVI-SR contains 24 unique video sequences at a range of spatial resolutions up to UHD-1 (3840p). These sequences were used as the basis for a large-scale subjective experiment exploring the relationship between visual quality and spatial resolution when using three distinct spatial adaptation filters (including a CNN-based super-resolution method). The results demonstrate that while spatial resolution has a significant impact on mean opinion scores (MOS), no significant reduction in visual quality between UHD-1 and HD resolutions for the superresolution method is reported. A selection of image quality metrics were benchmarked on the subjective evaluations, and analysis indicates that VIF offers the best performance.</p></li>
										<li><a href="FRQM/">FRQM: A Frame Rate Dependent Video Quality Metric</a>
                    <p>FRQM characterises the relationship between variations in frame rate and perceptual video quality. The proposed method estimates the relative quality of a low frame rate video with respect to its higher frame rate counterpart, through temporal wavelet decomposition, subband combination and spatiotemporal pooling. FRQM was tested alongside six commonly used quality metrics (two of which explicitly relate frame rate variation to perceptual quality), on the publicly available BVI-HFR video database, that spans a diverse range of scenes and frame rates, up to 120fps. Results show that FRQM offers significant improvement over all other tested quality assessment methods with relatively low complexity. </p></li>
			<li>    <a href="BVI-HFR/">BVI-HFR: A High Frame Rate Video Database</a>
                    <p>The BVI-HFR video database is a publicly available high frame rate video database, and contains 22 unique HD video sequences at frame rates up to 120 Hz. Subjective evaluations of 51 participants on the sequences in the BVI-HFR video database have shown a clear relationship between frame rate and perceived quality (MOS), although we do see the effect of diminishing returns.</p></li>
							
							<li><a href="WhatsonTV/">What's on TV: A Large-Scale Quantitative Characterisation of Modern Broadcast Video Content</a>
                    <p>In collaboration with the BBC, the  "What's on TV" is a large-scale analysis of the low-level features that exist in contemporary broadcast video. The project aims to establish an efficient set of features that can be used to characterise the spatial and temporal variation in modern consumer content. The meaning and relative significance of this feature set, together with the shape of their frequency distributions, represent highly valuable information for researchers wanting to model the diversity of modern consumer content in representative video databases.</p></li>
					
							<li><a href="BVI-HD/">BVI-HD: A Perceptual Video Quality Database for HEVC and Texture Synthesis Compressed Content</a>
                    <p>This is a new high definition video quality database, which contains 32 reference and 384 distorted video sequences plus subjective scores. They are also associated with subjective opinion scores collected from a total of 86 subjects, using a double stimulus test methodology. </p></li>
											
											<li><a href="Duration/">Optimal Presentation Duration for Video Quality Assessment</a>
                    <p>In this work, we explore the impact of reducing sequence length upon perceptual accuracy when identifying compression artifacts. We argue that sequences between 5 and 10s produce satisfactory levels of accuracy but the practical benefits of acquiring more data lead us to recommend the use of 5s sequences for future VQA studies that use the single and double stimulus continuous quality scale methodologies.</p></li>
					<li><a href="BVI-Texture/">BVI-Texture: A Video Texture Database for Perceptual Compression and Quality Assessment</a>
                    <p>BVI-Texture is a publicly available video texture database that contains test sequences and subjective opinion scores. The database exhibits a wide range of static and dynamic textures together with some mixed content. Each sequence is indexed using various video feature descriptors that characterize its spatial activity, temporal activity, static texture content and dynamic texture content. Moreover, rate/distortion results for the new dataset are presented after compression using HEVC, alongside subjective quality evaluation data. The BVI texture database will provide utility in testing quality assessment metrics and emerging video compression methods, particularly those based on texture analysis and synthesis.</p></li>
										
										<li><a href="VQA/">PVM: Perceptual Visual Quality Assessment</a>
                    <p>PVM is a novel perceptionbased hybrid model for video quality assessment, which simulates the HVS perception process by adaptively combining noticeable distortion and blurring artifacts using an enhanced nonlinear model. All stages of our model exploit the orientation selectivity and shift invariance properties of the dual-tree complex wavelet transform. This not only helps to improve the performance but also offers the potential for new low complexity in-loop application. </p></li>
					</ul>
			<b><font SIZE="4">Creative Technology</font></b>
			</font>
			<br>
			<ul>
			<li> <a href="Drone/">A Simulation Environment for Drone Cinematography</a>
                    <p>A workflow has been developed for the simulation of drone operations exploiting realistic background environments constructed within Unreal Engine 4 (UE4). This simulation tool will contribute to enhanced productivity, improved safety (awareness and mitigations for crowds and buildings), improved confidence of operators and directors and ultimately enhanced quality of viewer experience.</p></li>
			
						<li><a href="MultiDrone/">Drone Cinematography</a>
                    <p>In this project, we have refined the grammar of typical shot types based on our interviews and on experience gained during planning and shooting of live footage. A series of subjective experiments were also conducted based on simulation videos in order to evaluate optimum drone parameters in typical single drone shot types.</p></li>
						<font size=+1>
						</ul>
			<b><font SIZE="4">Perceptual Video Coding</font></b>
			</font>
			<br>
			<ul>
			<li>	<a href="RDO/">Rate Quality Optimisation: Lagrangian Optimisation</a>
                    <p>In this work, we conducted a comprehensive analysis of the results of a Lagrange multiplier selection experiment conducted on various video content using H.264/AVC and HEVC reference encoders. These results show that the original Lagrange multiplier selection methods, employed in both video encoders, are able to achieve optimum rate-distortion performance for I and P frames, but fail to perform well for B frames. The relationship is identified between the optimum Lagrange multipliers for B frames and distortion information obtained from the experimental results, leading to a novel Lagrange multiplier determination approach. </p></li>
					<li>	<a href="PVC/">Parametric Video Coding</a>
                    <p> This project developed parametric video coding algorithms which employs a perspective motion model to warp static textures and utilises texture synthesis to create dynamic textures. Texture regions are segmented using features derived from the complex wavelet transform and further classified according to their spatial and temporal characteristics. Moreover, a compatible artefact-based video metric (AVM) is proposed with which to evaluate the quality of the reconstructed video. This is also employed in-loop to prevent warping and synthesis artefacts. The proposed algorithm has been integrated into an H.264 video coding framework. The results show significant bitrate savings, of up to 60% compared with H.264 at the same objective quality (based on AVM) and subjective scores.</p></li>
					</ul>









					
			
					
            </div>



                <h3></h3>
            </div>
		</div>
    </div>        
</body>

</html>
