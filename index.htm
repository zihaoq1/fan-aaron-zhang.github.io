<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Dr Fan (Aaron) Zhang</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/5.0.0/normalize.min.css">
    <link rel="stylesheet" href="./css/style.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
</head>

<body id="page-top">
    <div class="page-container">
        <div class="inner">
            <div class="container">
                <div class="row">
                    <div class="row vertical-align">
                        <div class="col-md-3">
                            <div class="title">
							<br>
							<br>
                                <p><img src="Fan.jpg" width="100%" height="20%" class="center" /></p>
                            </div>
                        </div>
                        <div class="col-md-5">
                            <div class="title">
							<br>
							<br>
                                <h1>Dr Fan (Aaron) Zhang </h1>
								
								<b><font SIZE="4">Research Fellow</font></b><br>
								in Image and Video Communication<br>
								<br>
								1.23, 1 Cathedral Square<br>
                                Bristol BS1 5DD, United Kingdom<br>
                                fan.zhang@bristol.ac.uk<br>
								
								<ul class="list-inline list-social-icons mb-0">
								<br>
                                    <li class="list-inline-item">
                                        <a href="https://scholar.google.com/citations?hl=en&user=BBujJNcAAAAJ">
                                            <span class="fa-3x">
                                                <i class="ai ai-google-scholar-square"></i>
                                            </span>
                                        </a>
                                    </li>
                                    <li class="list-inline-item">
                                        <a href="https://www.linkedin.com/in/fan-zhang-b32ba430/">
                                            <span class="fa-3x">
												<i class="fab fa-linkedin"></i>
                                            </span>
                                        </a>
                                    </li>
									<li class="list-inline-item">
                                        <a href="https://www.github.com/fan-aaron-zhang">
                                            <span class="fa-3x">
                                                <i class="fab fa-github-square"></i>
                                            </span>
                                        </a>
                                    </li>
                                    <li class="list-inline-item">
                                        <a href="CV_2021.pdf">
                                            <span class="fa-3x">
                                                <i class="ai ai-cv-square"></i>
                                            </span>
                                        </a>
                                    </li>
                                </ul>
                            </div>
                        </div>
                        <div class="col-md-4">
						<br>
						<br>
						    <p><a href='https://www.bristol.ac.uk'><img src="uob-logo.svg" width="70%" height="20%" alt=""></a></p>
                            <p><a href='https://www.bristol.ac.uk/vision-institute'><img src="bvilogo.png" width="65%" height="20%" alt=""></a></p>										
                            <p><a href='https://vilab.blogs.bristol.ac.uk'><img src="VILogo.jpg" width="80%" height="20%" alt=""></a></p>


                        </div>
                    </div>
                </div>
            </div>
			<br>
            <div class="container-md">
                <h2>About</h2> I am currently working as a Research Fellow in the <A 
HREF="http://www.bristol.ac.uk/vi-lab/">Visual Information Laboratory</A>, which is led by <A HREF="http://www.bristol.ac.uk/eeng/department/staff/drb.html">Prof. David Bull</A>, within the <A 
HREF="http://www.bris.ac.uk/eeng/">Department of Electrical and Electronic 
Engineering</A>, <A HREF="http://www.bris.ac.uk/">University of Bristol</A>. I have won and worked within projects on AI-enhanced video compression, video quality assessment, perceptual video coding, intelligent creative technologies and drone cinematography.
            </div>
            <hr />
			            <div class="container-md">
                <h2>Research Areas</h2> 
				 <div class="row">
                    <div class="row vertical-align">
				 <div class="col-md-5">
				<ul>
                    <li><b><font SIZE="4">Deep Video Coding</font></b></li>
					<ul>
						<li>AI-enhanced deep video compression</li>
						<li>New CNN architectures for compression and perceptual loss functions</li>
						<li>New training methods and databases</li>
						<li>Reduced complexity architectures</li>
                    </ul>
                </ul>
				</div>
				
				<div class="col-md-5">
				<ul>
                    <li><b><font SIZE="4">Creative Technology</font></b></li>
					<ul>
						<li>Video production workflows</li>
						<li>Immersive (HDR/HFR/HSR/360&deg/volumetric) video coding and quality assessment</li>
						<li>New training methods and databases</li>
						<li>Reduced complexity architectures</li>
                    </ul>
                </ul>
				</div>

				 <div class="col-md-5">
				<ul>
                    <li><b><font SIZE="4">Video Quality Assessment</font></b></li>
					<ul>
						<li>Full reference/reduce reference quality metrics</li>
						<li>Machine learning based quality metrics</li>
						<li>HVS property characterisation and subjective quality assessment</li>
						<li>Crowd-sourced subjective testing</li>
                    </ul>
                </ul>
				</div>
				<div class="col-md-5">
				<ul>
                    <li><b><font SIZE="4">Perceptual Video Coding</font></b></li>
					<ul>
						<li>Content-aware compression</li>
						<li>Rate quality optimisation and adaptive RDO/quantisation</li>
						<br>
						<br>
                    </ul>
				</ul>
				</div>
				</div>
            </div>        
            <hr />
            <div class="container-md">
                <h2 id="projects">News & Activities</h2>
                <ul>
					<li><b><font SIZE="4">Jan 2021:</font></b> Our paper on <A HREF="https://arxiv.org/abs/2009.07583">"Video Compression with CNN-based Post Processing"</A> has been accepted by the IEEE MultiMedia Magazine.</li>
					<li><b><font SIZE="4">Dec 2020:</font></b> I join the editorial board of <A HREF="https://ieee-cas.org/publications/transactions-circuits-and-systems-video-technology">IEEE T-CSVT </A>as an associate editor (2021-2022).</li>
					<li><b><font SIZE="4">Nov 2020:</font></b> Our paper on <A HREF="https://arxiv.org/abs/2007.07099">"MFRNet: A New CNN Architecture for Post-Processing and In-loop Filtering"</A> has been accepted by the IEEE Journal of Selected Topics in Singal Processing <A HREF="https://signalprocessingsociety.org/blog/ieee-jstsp-special-issue-deep-learning-imagevideo-restoration-and-compression">Special Issue</A> on Deep Learning for Image/Video Restoration and Compression.</li>
					<li><b><font SIZE="4">Apr 2020:</font></b> We have released a large video database, <a href="BVI-DVC/index.htm">BVI-DVC</a>, for training deep learning based video coding algorithms. It has been identified by <a href="https://jvet-experts.org/doc_end_user/current_document.php?id=10484">MPEG JVET AHG11</a> (neural network-based video coding) as one of their training datasets.</li>
                    <li><b><font SIZE="4">Jan 2020:</font></b> Organising committee (Finance) <a href="https://pcs2021.org/">PCS 2021</a></li>
                </ul>
				<b><font SIZE="4"><a href="news.htm">[All news and activities]</a></font></b>
            </div>
            <hr />
            <div class="container-md">
                <h2 id="projects">Recent Research Projects</h2>
            <div class="researchlist">
			
			<div class="row">
            <div class="row vertical-align">
			<div class="col-md-4">
                            <div class="title">
                                <p><img src="CVE-GAN/mul2res2.svg" width="65%" class="center" /></p>
                            </div>
                        </div>
						<div class="col-md-8">
                            			<a href="CVE-GAN/">CVEGAN: A Perceptually-inspired GAN for Compressed Video Enhancement</a>
                    <p>CVEGAN is a new Generative Adversarial Network for Compressed Video quality Enhancement (CVEGAN). The generator benefits from the use of a novel Mul2Res block (with multiple levels of residual learning branches), an enhanced residual non-local block (ERNB) and an enhanced convolutional block attention module (ECBAM). The ERNB has also been employed in the discriminator to improve the representational capability. The training strategy has also been re-designed specifically for video compression applications, to employ a relativistic sphere GAN (ReSphereGAN) training methodology together with new perceptual loss functions. </p>
                        </div>	
					</div>	
					</div>	 
						
                    <div class="line_sep"></div>
								<div class="row">
            <div class="row vertical-align">
			<div class="col-md-4">
                            
                                <p><img src="CNN-PP/framework.svg" width="100%" class="center" /></p>
                                                    </div>
						<div class="col-md-8">
                            						<a href="CNN-PP/">Video Compression with CNN-based Post Processing</a>
                    <p>This is a new CNN-based post-processing approach, which has been integrated with two state-of-the-art coding standards, VVC and AV1. It offers consistent coding gains on all tested sequences at various spatial resolutions, with average bit rate savings of 4.0% and 5.8% against original VVC and AV1 respectively (based on the assessment of PSNR). This network has also been trained with perceptually inspired loss functions, swhich have further improved reconstruction quality based on perceptual quality assessment (VMAF), with average coding gains of 13.9% over VVC and 10.5% against AV1.</p>
                        </div>	
					</div>	
					</div>	 
					
								<div class="row">
            <div class="row vertical-align">
			<div class="col-md-4">
                            
                                <p><img src="Drone/SimulationOptions.jpg" width="100%" height="20%" class="center" /></p>
                            
                        </div>
						<div class="col-md-8">
                            						<a href="Drone/">A Simulation Environment for Drone Cinematography</a>
                    <p>A workflow has been developed for the simulation of drone operations exploiting realistic background environments constructed within Unreal Engine 4 (UE4). This simulation tool will contribute to enhanced productivity, improved safety (awareness and mitigations for crowds and buildings), improved confidence of operators and directors and ultimately enhanced quality of viewer experience.</p>
                        </div>	
					</div>	
					</div>

								<div class="row">
            <div class="row vertical-align">
			<div class="col-md-4">
                            
                                <p><img src="ViSTRA/framework_new.svg" width="100%" class="center" /></p>
                            
                        </div>
						<div class="col-md-8">
                            						<a href="ViSTRA/">ViSTRA: Video Compression based on Resolution Adaptation</a>
                    <p>ViSTRA a new video compression framework which exploits adaptation of spatial/temporal resolution and effective bit depth, down-sampling these parameters at the encoder based on perceptual criteria, and up-sampling at the decoder using a deep convolution neural network.</p>
                        </div>	
					</div>	
					</div>
			 
				<b><font SIZE="4"><a href="research.htm">[All research projects]</a></font></b>
			
					
            </div>



                <h3></h3>
            </div>
            <hr />
            <div class="container-md">
                <h2 id="projects">Publication</h2>

<td valign=top>
<font size=+1>
<b><font SIZE="4">Book and Book Chapters</font></b>
</font>

<ol start="1">
<li>Intelligent Image and Video Compression: Communicating Pictures. <a href='https://www.amazon.co.uk/Image-Video-Compression-Communicating-Pictures/dp/0128203536/'>[book]</a><a href="Intelligent-Image-and-Video-Compression.htm">[software]</a>
<br>
D. Bull and F. Zhang, <em>2nd Edition</em>, Oxford: Academic Press, in Press.
<br>
<li>Measuring video quality. <A HREF="http://www.sciencedirect.com/science/article/pii/B9780124201491000077">[book]</A>
<br>
F. Zhang and D. Bull, <em>In: Sergios Theodoridis and Rama Chellappa, editors, Academic Press Library in Signal Processing. Vol 5. </em>, Oxford: Academic Press, 2014, pp 227-249. ISBN: 978-0-12-420149-1.
<br>
</td>
</ol>




<td valign=top>
<font size=+1>
<b><font SIZE="4">MPEG Standard Contributions</font></b>
</font>


<ol start="1">
<li> <A HREF="http://phenix.it-sudparis.eu/jvet/doc_end_user/current_document.php?id=3432">Description of SDR video coding technology proposal by University of Bristol (JVET-J0031)</A>
<br>
D. Bull, F. Zhang and M. Afonso, <em>A submission to the Joint Call for Proposals on Video Compression with Capability beyond HEVC,</em> April 2018 in San Diego.
<br>
<p>
<li> <A HREF="http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=10306">BVI_Texture UHD 120fps test sequences for HEVC and beyond (JCTVC-V0099)</A>
<br>
M. Papadopoulos, F. Zhang, D. Agrafiotis, D. Bull and J.-R. Ohm,</em> October 2015 in Geneva.
<br>
<p>
</td>
</ol>



<td valign=top>
<font size=+1>
<b><font SIZE="4">arXiv Papers</font></b>
</font>


<ol start="1">
<li>Enhancing VMAF through New Feature Integration and Model Combination. <A HREF="https://arxiv.org/abs/2103.06338">[paper]</A>
<br>
Fan Zhang, Angeliki Katsenou, Christos Bampis, Lukas Krasula, Zhi Li and David Bull, arXiv:2103.06338, 2021.
<br>
<p>
<li>A Subjective Study on Videos at Various Bit Depths. <A HREF="https://arxiv.org/abs/2103.10363">[paper]</A>
<br>
Alex Mackin, Di Ma, Fan Zhang and David Bull, arXiv:2103.10363, 2021.
<br>
<p>
<li>VMAF-based Bitrate Ladder Estimation for Adaptive Streaming. <A HREF="https://arxiv.org/abs/2103.07564">[paper]</A>
<br>
Angeliki V. Katsenou, Fan Zhang, Kyle Swanson, Mariana Afonso, Joel Sole and David R. Bull, arXiv:2103.07564, 2021.
<br>
<p>
<li> CVEGAN: a perceptually-inspired GAN for compressed video enhancement. <A HREF="https://arxiv.org/abs/2011.09190">[paper]</A>
<br>
D. Ma, F. Zhang, and D. R. Bull, arXiv:2011.09190, 2020
<br>
<p>
<li> BVI-DVC: A Training Database for Deep Video Compression. <A HREF="https://arxiv.org/abs/2003.13552">[paper]</A><a href="BVI-DVC/index.htm">[dataset]</a>
<br>
D. Ma, F. Zhang, and D. R. Bull, arXiv:2003.13552, 2020
<br>
<p>
<li> Comparing VVC, HEVC and AV1 using Objective and Subjective Assessments. <A HREF="https://arxiv.org/abs/2003.10282">[paper]</A><a href="https://vilab.blogs.ilrt.org/?p=2295">[dataset]</a>
<br>
F. Zhang, A. V. Katsenou, M. Afonso, G. Dimitrov and D. R. Bull, arXiv:2003. 10282, 2020.
<br>
<p>
<li>ViSTRA2: Video Coding using Spatial Resolution and Effective Bit Depth Adaptation. <A HREF="https://arxiv.org/abs/1911.02833">[paper]</A><a href="https://vilab.blogs.bristol.ac.uk/?p=2278">[project]</a>
<br>
F. Zhang, M. Afonso and D. R. Bull, arXiv:1911.02833, 2019
<p>
</td>
</ol>

<td valign=top>
<font size=+1>
<b><font SIZE="4">Journal Papers</font></b>
</font>
<ol start="1">
<li> Video Compression with CNN-based Post Processing. <A HREF="https://arxiv.org/abs/2009.07583">[paper]</A><a href="CNN-PP/index.htm">[project]</a>
<br>
F. Zhang, D. Ma, C. Feng and D. R. Bull, <em>IEEE MultiMedia Magazine</em>, accepted.
<br>
<p>
<li> MFRNet: A New CNN Architecture for Post-Processing and In-loop Filtering. <A HREF="https://arxiv.org/abs/2007.07099">[paper]</A>
<br>
D. Ma, F. Zhang, and D. R. Bull, <em>IEEE Journal of Selected Topics in Singal Processing</em>, accepted.
<br>
<p>
<li>A Study of High Frame Rate Video Formats. <A HREF="https://ieeexplore.ieee.org/document/8531714">[paper]</A><a href="https://vilab.blogs.ilrt.org/?p=1563">[project]</a><a href="BVI-DVC/index.htm">[dataset]</a>
<br>
A. Mackin, F. Zhang, and D. R. Bull, <em>IEEE T-MM,</em> 2019.
<br>
<p>
<li>Video Compression based on Spatio-Temporal Resolution Adaptation. <A HREF="https://ieeexplore.ieee.org/document/8517114">[paper]</A><a href="ViSTRA/">[project]</a>
<br>
M. Afonso, F. Zhang and D. R. Bull, <em>IEEE T-CSVT (Letter),</em> 2019.
<br>
<p>
<li>Rate-distortion Optimization Using Adaptive Lagrange Multipliers.  <A HREF="https://ieeexplore.ieee.org/document/8482327">[paper]</A><a href="RDO/">[project]</a>
<br>
F. Zhang and D. R. Bull, <em>IEEE T-CSVT,</em> 2019.
<br>
<p>
<li>BVI-HD: A Video Quality Database for HEVC Compressed and Texture Synthesised Content. <A HREF="https://ieeexplore.ieee.org/document/8322297">[paper]</A><a href="BVI-HD/">[dataset]</a>
<br>
F. Zhang, F. Mercer Moss, R. Baddeley and D. R. Bull, <em>IEEE T-MM,</em> 2018.
<br>
<p>
<li>Reduced-Reference Video Quality Metric Using Spatial Information in Salient Regions.  <A HREF="http://journal.uad.ac.id/index.php/TELKOMNIKA/article/view/9036/pdf_679">[paper]</A>
<br>
F. D. A. Rahman, D. Agrafiotis, O. O. Khalifa and F. Zhang, <em>TELKOMNIKA (Telecommunication Computing Electronics and Control),</em> 2018.
<br>
<p>
<li>On the Optimal Presentation Duration for Subjective Video Quality Assessment.  <A HREF="https://ieeexplore.ieee.org/document/7172512">[dataset]</A><a href="Duration/index.htm">[project]</a>
<br>
F. Mercer Moss, K. Wang, F. Zhang, R. Baddeley and D. Bull, <em>IEEE T-CSVT,</em> November 2016.
<br>
<p>
<li>Support for Reduced Presentation Durations in Subjective Video Quality Assessment. <A HREF="http://www.sciencedirect.com/science/article/pii/S0923596516301126">[paper]</A><a href="Duration/index.htm">[project]</a>
<br>
F. Mercer Moss, C.-T. Yeh, F. Zhang, R. Baddeley, D. R. Bull, <em>Elsevier Signal Processing: Image Communication,</em> October 2016.
<br>
<p>
<li>A Perception-based Hybrid Model for Video Quality Assessment <A HREF="https://ieeexplore.ieee.org/document/7100892">[paper]</A><a href="VQA/PVM_code.zip">[code]</a><a href="VQA/index.htm">[project]</a>
<br>
F. Zhang and D. Bull, <em>IEEE T-CSVT,</em> June 2016.
<br>
<p>
<li>Perception-oriented Video Coding based on Image Analysis and Completion: a Review. <A HREF="http://www.sciencedirect.com/science/article/pii/S0923596512000100">[paper]</A>
<br>
P. Ndjiki-Nya, D. Doshkov, H. Kaprykowsky, F. Zhang, D. Bull, T. Wiegand, <em>Signal Processing: Image Communication</em>, July 2012.
<br>
<p>
<li>A Parametric Framework For Video Compression Using Region-based Texture Models. <A HREF="https://ieeexplore.ieee.org/document/5986673">[paper]</A><a href="PVC/">[project]</a>
<br>
F. Zhang and D. Bull, <em>IEEE J-STSP</em>, November 2011.
<br>
</td>
</ol>


<td valign=top>
<font size=+1>
<b><font SIZE="4">Conference Contributions</font></b>
</font>

<ol start="1">
<li>Enhancing VVC through CNN-based Post-Processing. <A HREF="https://ieeexplore.ieee.org/document/9102912">[paper]</A><a href="CNN-PP/index.htm">[project]</a>
<br>
F. Zhang, C. Feng and D. Bull, <em>ICME,</em> 2020.
<br>
<p>
<li>A Simulation Environment for Drone Cinematography. <A HREF="https://www.ibc.org/technical-papers/a-simulation-environment-for-drone-cinematography/6747.article">[paper]</A>
<br>
F. Zhang, D. Hall, T. Xu, S. Boyle and D. Bull, <em>IBC,</em> 2020.
<br>
<p>
<li>Video compression with low complexity CNN-based spatial resolution adaptation. <A HREF="https://arxiv.org/abs/2007.14726">[paper]</A>
<br>
D. Ma, F. Zhang, and D. Bull, <em>SPIE,</em> 2020.
<br>
<p>
<li>GAN-based Effective Bit Depth Adaptation for Perceptual Video Compression. <A HREF="https://ieeexplore.ieee.org/document/9102865">[paper]</A>
<br>
D. Ma, F. Zhang and D. Bull, <em>ICME,</em> 2020.
<br>
<p>
<li>Encoding in the Dark Grand Challenge: An Overview. <A HREF="https://ieeexplore.ieee.org/document/9106011">[paper]</A>
<br>
N. Anantrasirichai, F. Zhang, A. Malyugina, P. Hill, A. Katsenou, <em>ICME Workshops,</em> 2020.
<br>
<p>
<li>Enhanced Video Compression based on Effective Bit Depth Adaptation. <A HREF="https://ieeexplore.ieee.org/document/8803185">[paper]</A>
<br>
F. Zhang, M. Afonso and D. Bull, <em>ICIP,</em> 2019.
<br>
<p>
<li>A Subjective Study of Viewing Experience for Drone VIdeos Using Simulated Content. <A HREF="https://ieeexplore.ieee.org/document/8803747">[paper]</A>
<br>
S. Boyle, F. Zhang and D. Bull, <em>ICIP,</em> 2019.
<br>
<p>
<li>A Subjective Comparison of AV1 and HEVC for Adaptive Video Streaming. <A HREF="https://ieeexplore.ieee.org/document/8803523">[paper]</A><a href="https://vilab.blogs.ilrt.org/?p=2295">[dataset]</font></a>
<br>
A. Katsenou, F. Zhang, M. Afonso and D. Bull, <em>ICIP,</em> 2019.
<br>
<p>
<li>Frame Rate Conversion Method based on a Virtual Shutter Angle. <A HREF="https://ieeexplore.ieee.org/document/8803489">[paper]</A>
<br>
A. Mackin, F. Zhang and D. Bull, <em>ICIP,</em> 2019.
<br>
<p>
<li>Environment Capture and Simulation for UAV Cinematography Planning and Training. <A HREF="http://eusipco2019.org/wp-content/uploads/2019/08/Environment_Capture_and_Simulation_for_UAV_Cinematography_Planning_and_Training.pdf">[paper]</A>
<br>
S. Boyle, M. Newton, F. Zhang and D. Bull, <em>EUSIPCO,</em> 2019.
<br>
<p>
<li>Perceptually-inspired Super-resolution of Compressed Videos. <A HREF="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11137/1113717/Perceptually-inspired-super-resolution-of-compressed-videos/10.1117/12.2530688.short">[paper]</A>
<br>
D. Ma, M. Afonso, F. Zhang and D. Bull, <em>SPIE,</em> 2019.
<br>
<p>
<li>The Future of Media Production Through Multi-drones' Eyes. <A HREF="https://personal.us.es/jcapitan/preprint/messina_ibc18_web.pdf">[paper]</A>
<br>
A. Messina, S. Metta, M. Montagnuolo, F. Negro, V. Mygdalis, I. Pitas, J. Capitan, A. Torres, S. Boyle, D. Bull and F. Zhang, <em>IBC,</em> 2018.
<br>
<p>
<li>A study of subjective video quality at various spatial resolutions. <A HREF="https://ieeexplore.ieee.org/document/8451225">[paper]</A><a href="BVI-SR/">[dataset]</a>
<br>
A. Mackin, M. Afonso, F. Zhang and D. Bull, <em>ICIP,</em> 2018.
<br>
<p>
<li>Spatial resolution adaptation framework for video compression. <A HREF="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10752/107520L/Spatial-resolution-adaptation-framework-for-video-compression/10.1117/12.2320520.short?SSO=1">[paper]</A>
<br>
M. Afonso, F. Zhang and D. Bull, <em>SPIE,</em> 2018.
<br>
<p>
<li>SRQM: A Video Quality Metric for Spatial Resolution Adaptation. <A HREF="https://ieeexplore.ieee.org/document/8456246">[paper]</A><a href="https://vilab.blogs.bristol.ac.uk/?p=2180">[code]</a>
<br>
A. Mackin, M. Afonso, F. Zhang and D. Bull, <em>PCS,</em> 2018.
<br>
<p>
<li>A Frame Rate Dependent Video Quality Metric based on Temporal Wavelet Decomposition and Spatiotemporal Pooling. <A HREF="http://ieeexplore.ieee.org/document/8296291">[paper]</A><a href=https://vilab.blogs.bristol.ac.uk/?p=2265">[code]</a>
<br>
F Zhang, A Mackin and D. R. Bull, <em>ICIP,</em> 2017.
<br>
<p>
<li>Low Complexity Video Coding Based on Spatial Resolution Adaptation. <A HREF="http://ieeexplore.ieee.org/document/8296835">[paper]</A>
<br>
M. Afonso, F. Zhang, A. Katsenou, D. Agrafiotis, D. Bull, <em>ICIP,</em> 2017.
<br>
<p>
<li>Investigating the Impact of High Frame Rates on Video Compression. <A HREF="https://ieeexplore.ieee.org/document/8296290">[paper]</A><a href="https://data.bris.ac.uk/data/dataset/k8bfn0qsj9fs1rwnc2x75z6t7">[dataset]</a>
<br>
A. Mackin, F. Zhang, M. A. Papadopoulos, D. Bull, <em>ICIP,</em> 2017.
<br>
<p>
<li>Video Texture Analysis based on HEVC Encoding
Statistics. <A HREF="https://ieeexplore.ieee.org/document/7906312">[paper]</A><a href="https://data.bris.ac.uk/data/dataset/1h2kpxmxdhccf1gbi2pmvga6qp">[dataset]</a>
<br>
M. Afonso, A. Katsenou, F. Zhang, D. Agrafiotis, D. Bull, <em>PCS,</em> 2016.
<br>
<p>
<li>HEVC Enhancement using Content-based Local QP Selection. <A HREF="https://ieeexplore.ieee.org/document/7533154">[paper]</A>
<br>
F. Zhang and D. Bull, <em>ICIP,</em> 2016.
<br>
<p>
<li>An Adaptive QP Offset Determination Method for HEVC. <A HREF="https://ieeexplore.ieee.org/document/7533155">[paper]</A>
<br>
 M. A. Papadopoulos, F. Zhang, D. Agrafiotis, D. R. Bull, <em>ICIP,</em> 2016.
<br>
<p>
<li>What's on TV: A Large Scale Quantitative Characterisation of Modern Broadcast Video Content. <A HREF="https://ieeexplore.ieee.org/document/7532794">[paper]</A><a href="https://vilab.blogs.ilrt.org/?p=1554">[project]</a>
<br>
F. Mercer Moss, F. Zhang, R. Baddeley, D. R. Bull, <em>ICIP,</em> 2016.
<br>
<p>
<li> An Adaptive Lagrange Multiplier Determination Method for Rate-distortion Optimisation in Hybrid Video Codecs. <A HREF="https://ieeexplore.ieee.org/document/7350883">[paper]</A><a href="https://vilab.blogs.ilrt.org/?p=2268">[project]</a>
<br>
F. Zhang and D. Bull, <em>ICIP,</em> 2015.
<br>
<p>
<li>A Study of Subjective Video Quality at Various Frame Rates.  <A HREF="https://ieeexplore.ieee.org/document/7351436">[paper]</A><a href="BVI-HFR/index.htm">[dataset]</a>
<br>
A. Mackin, F. Zhang and D. Bull, <em>ICIP,</em> 2015.
<br>
<p>
<li>A Very Low Complextiy Reduced Reference Video Quality Metric based on Spatio-temporal Information Selection. <A HREF="https://ieeexplore.ieee.org/document/7350863">[paper]</A>
<br>
M. Wang, F. Zhang and D. Agrafiotis, <em>ICIP,</em> 2015.
<br>
<p>
<li>A Video Texture Database for Perceptual Compression and Quality Assessment. <A HREF="https://ieeexplore.ieee.org/document/7351309">[paper]</A><a href="https://data.bris.ac.uk/datasets/1if54ya4xpph81fbo1gkpk5kk4/">[dataset]</a>
<br>
M. A. Papadopoulos, F. Zhang, D.Agrafiotis and D. Bull, <em>ICIP,</em> 2015.
<br>
<p>
<li>Optimal sequence duration for subjective video quality assessment. <A HREF="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2444238">[paper]</A><a href="Duration/index.htm">[project]</a>
<br>
F. J. Mercer Moss, K. Wang, F. Zhang, R. Baddeley and D. Bull, <em>SPIE Optical Engineering+ Applications,</em> 2015.
<br>
<p>
<li>Quality Assessment Methods for Perceptual Video Compression. <A HREF="https://ieeexplore.ieee.org/document/6738009">[paper]</A><a href="http://vilab.blogs.ilrt.org/files/2016/06/PVM_code.zip">[code]</a><a href="https://vilab.blogs.ilrt.org/?p=787">[project]</a>
<br>
F. Zhang and D. Bull, <em>ICIP,</em> Melbourne, Australia, September 2013.
<br>
<p>
<li>Production of high dynamic range video. <A HREF="http://www.bbc.co.uk/rd/publications/whitepaper269">[paper]</A>
<br>
M. Price, D. Bull, T. Flaxton, S. Hinde, R. Salmon, A. Sheikh, G. Thomas, and F. Zhang, <em>IBC</em>, Amsterdam, September, 2013.
<br>
<p>
<li>Advances in Region-based Texture Modeling for Video Compression. <A HREF="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1267237">[paper]</A><a href="https://vilab.blogs.ilrt.org/?page_id=553">[project]</a>
<br>
F. Zhang and D. Bull, <em>Proc. SPIE 8135</em>, San Diego, USA, August, 2011.
<br>
<p>
<li> Enhanced Video Compression With Region-Based Texture Models. <A HREF="https://ieeexplore.ieee.org/document/5702560">[paper]</A><a href="https://vilab.blogs.ilrt.org/?page_id=553">[project]</a>
<br>
F. Zhang and D. Bull, <em>PCS</em>, Nagoya, Japan, December, 2010.
<br>
<p>
<li> Region-Based Texture Modelling For Next Generation Video Codecs <A HREF="https://ieeexplore.ieee.org/document/5651626">[paper]</A><a href="https://vilab.blogs.ilrt.org/?page_id=553" >[project]</a>
<br>
F. Zhang, D. Bull, and N. Canagarajah, <em>ICIP</em>, Hong Kong, China, September, 2010.
<br>
</td>
</ol>

<br>
<td valign=top>
<font size=+1>
<b><font SIZE="4">Other Publications</font></b>
</font>

<ol start="1">
<li> Exploring the Challenges of Higher Frame Rates:
from Quality Assessment to Frame Rate Selection. <A HREF="http://mmc.committees.comsoc.org/files/2018/07/01-MMTC_Communication_Frontier_May_2018-Final-Revised.pdf">[paper]</A><a href="https://vilab.blogs.ilrt.org/?p=1563">[project]</a>
<br>
A. V. Katsenou, A. Mackin, D. Ma, F. Zhang and D. R. Bull, <em>IEEE COMSOC MMTC Communications - Frontiers (E-Letter),</em> May 2018 (invited).
<br>
<p>
</ol>
</td>


<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11440198; 
var sc_invisible=0; 
var sc_security="e7e7610e"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="web analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11440198/0/e7e7610e/0/" alt="web
analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
		
            </div>
            <hr />
        </div>
    </div>
</body>

</html>
